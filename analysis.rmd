---
title: "Analysis of Genre Audio Features"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    css: dashboard-styles.css
    theme: simplex
    social: menu
    source: embed
---


```{r setup, include=FALSE}
library(flexdashboard)
library(spotifyr)
library(dplyr)
library(ggplot2)
library(reshape2)
library(tidyr)
library(ggforce)
library(tibble)
library(purrr)
library(compmus)
library(randomForest)
library(caret)
library(corrplot)
library(pROC)
```



```{r}	
Sys.setenv(SPOTIFY_CLIENT_ID = Sys.getenv("SPOTIFY_CLIENT_ID"))
Sys.setenv(SPOTIFY_CLIENT_SECRET = Sys.getenv("SPOTIFY_CLIENT_SECRET"))
access_token <- get_spotify_access_token()

# Fetching data
playlist_id <- "1RhRAqYJA1mwmBtk4mXRju"
username <- "quintijn.kroesbergen"
all_audio_features_df <- get_playlist_audio_features(username, playlist_uris = c(playlist_id), authorization = access_token)
```

```{r}
# Assigning genres
all_audio_features_df$genre <- NA
all_audio_features_df$genre[1:107] <- 'Afro House'
all_audio_features_df$genre[108:206] <- 'Deep House'
all_audio_features_df$genre[207:284] <- 'Melodic House'


muted_colors <- c('Afro House' = '#377eb8',  # Muted blue
                  'Deep House' = '#e41a1c',  # Muted red
                  'Melodic House' = '#4daf4a')  # Muted green

```


# Introduction of Corpus

My corpus is a collection with songs of three different genres, namely: Afro House, Deep House, and Melodic House. These genres are all subgenres of house, which makes them interesting to study to find out what distinguishes these genres. Each genre has its own unique blend of rhythm, melodies, and textures which make them very suitable for analysis.

Afro House, with its rhythmic energy and African influences, contrasts with Melodic House's emotive soundscapes and Deep House's soulful grooves. This selection aims to explore the auditory and emotional distinctions and intersections among these genres. I expect differences in their rhythmic structure; Afro House has more complex rhythms, whereas Deep House and Melodic House have simpler rhythms. The mood of the genres also differs, from Deep House's mellow vibes to Afro House's energy and Melodic House's ethereal qualities, yet their base structure is similar.

My corpus consists of dozens of songs for every genre, aiming for a diverse range of artists and songs that represent each genre. However a potential gap in my corpus is the focus on relatively recent releases which does not take the history of each genre into account. Also my corpus does potentially miss niche songs that could offer additional insights into each genre.

Typical songs for each genre are: Ben Bohmer - Beyond Beliefs for Melodic House, Rampa - Champion for Afro House, and Mahalo - Home for Deep House, embodying each genre's core values. Atypical songs like Notre Dame - Yumi - edit for Melodic House, Sebjak - Somebody - edit for Afro House, and Hannah Laing - Good Love for Deep House, are chosen for their unusual rhythm and tempo, offering additional insights into each genre.








# Harmony in Motion: Exploring Genre Energies and Emotions
## Visualization and Description of first visualisation

Column {data-width=750 .plot-column}
-------

```{r, align='center'}	

# Plotting energy vs valence for each genre in separate panels
ggplot(all_audio_features_df, aes(x = energy, y = valence)) +
  geom_point(aes(color = genre)) +  # Color points by genre
  facet_wrap(~ genre) +  # Create a separate plot for each genre
  labs(title = "Energy vs Valence for Each Genre",
       x = "Energy",
       y = "Valence",
       color = "Genre") +
  scale_color_manual(values = c('Afro House' = 'blue', 'Deep House' = 'red', 'Melodic House' = 'green')) +
  theme_minimal() +
  theme(legend.position = "right",
        strip.background = element_blank(),  
        strip.text = element_text(size = 12))  
genre_stats <- all_audio_features_df %>%
  group_by(genre) %>%
  summarise(
    energy_mean = mean(energy),
    energy_sd = sd(energy),
    valence_mean = mean(valence),
    valence_sd = sd(valence)
  )
```

Column {data-width=500 .discussion-column}
-------

This scatterplot shows how energy and emotional 'mood' vary across Afro House, Deep House, and Melodic House. Energy is on the horizontal axis, and 'valence' (meaning a track's positivity) is on the vertical.

Afro House has a cluster of points with medium to high energy, and its mood ranges widely.  This means Afro House can be anything from low-key to super upbeat, all while keeping its signature rhythm.

Deep House (in red) tends to stay high-energy, but with a medium emotional range. This means it's consistently good for dancing, with moods from neutral to quite positive.

Melodic House is different, with mostly lower energy and mood levels. This suggests it's more about thoughtful listening than getting pumped up.

Overall, this chart shows how each subgenre has its own feel. Afro House is the most versatile, Deep House is reliably danceable, and Melodic House offers a deeper, more reflective experience.



# Key Signatures and Genre Dynamics
## Visualisation

Column {data-width=500 .plot-column}
-------


```{r}

key_names <- c('C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B')
all_audio_features_df$key_name <- key_names[all_audio_features_df$key + 1]

# Calculate the count of tracks in each key by genre
counts <- all_audio_features_df %>%
  group_by(genre, key_name) %>%
  summarise(count = n(), .groups = 'drop')

# Calculate the total count of tracks in each genre
total_counts <- counts %>%
  group_by(genre) %>%
  summarise(total = sum(count), .groups = 'drop')

# Join the counts with total counts and calculate the proportion
counts <- counts %>%
  left_join(total_counts, by = "genre") %>%
  mutate(proportion = count / total)

# Calculate the most popular key for each genre
most_popular_keys <- counts %>%
  group_by(genre) %>%
  top_n(1, wt = proportion) %>%
  ungroup() %>%
  select(genre, key_name, proportion)


ymax_limit <- max(counts$proportion) * 1.3


# Create the histogram plot
p <- ggplot(counts, aes(x = key_name, y = proportion, fill = genre)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(title = "Normalized Distribution of Keys by Genre",
       x = "Musical Key",
       y = "Proportion of Tracks") +
  scale_fill_manual(values = muted_colors) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  ylim(0, ymax_limit)  # Extend y-axis limit

# Add annotations for the most popular keys
p + geom_text(data = most_popular_keys, aes(x = key_name, y = proportion, label = key_name),
              position = position_dodge(width = 0.9), vjust = -0.5, color = "black", fontface = "bold")




```


Column {data-width=500 .discussion-column}
-------

The analysis of the normalized distribution of musical keys across Afro House, Deep House, and Melodic House genres reveals some intriguing patterns. C# emerges as the predominant key in both Afro House and Melodic House, indicating a preference for its sound qualities in these genres. The choice of key can affect the mood and energy of the music. C#, being a semi-tone higher than C, is often considered to have a bright and lively quality which might resonate well with the energetic and emotive nature of Afro House and the ethereal, uplifting atmosphere of Melodic House.

In contrast, G is the most popular key in Deep House, which may underscore the genre's tendency towards a deeper, more mellow sonic palette. The key of G is often associated with a warm and inviting tone, which complements Deep House's soulful and laid-back vibe. This key can provide a comfortable harmonic foundation for the smooth grooves and relaxed rhythms characteristic of this genre.

The preference for these keys suggests that artists and producers are likely to lean towards certain tonalities that align with the expressive needs of their genre. While this trend does not dictate the creative choices made in music production, it highlights the subtle ways in which key selection can contribute to genre-specific atmospheres and listener expectations.



# The Heartbeat of House: Tempo as Genre Signature
## Visualisation

Column {data-width=500 .plot-column}
-------

```{r}

mode_tempi <- all_audio_features_df %>%
  group_by(genre, tempo) %>%
  summarise(count = n(), .groups = 'drop') %>%
  # Get the row with the maximum count for each genre
  slice_max(order_by = count, n = 1) %>%
  ungroup()


# Create the histogram plot
tempo_histogram <- ggplot(all_audio_features_df, aes(x = tempo, fill = genre)) +
  geom_histogram(binwidth = 2, alpha = 0.6, position = "identity") +  # Adjust binwidth as necessary
  labs(title = "Histogram of Tempi by Genre",
       x = "Tempo (BPM)",
       y = "Count of Tracks") +
  scale_fill_manual(values = muted_colors) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  facet_wrap(~ genre, scales = "free_y")  # Separate histogram for each genre

# Print the plot
print(tempo_histogram)


```

```{r}

# Calculate the average BPM for each genre
average_bpm <- all_audio_features_df %>%
  group_by(genre) %>%
  summarize(average_tempo = mean(tempo, na.rm = TRUE)) %>%
  ungroup()

# Create the text output for the average BPM in each genre
average_bpm_text <- average_bpm %>%
  mutate(text = paste("<b>", genre, "average BPM:", round(average_tempo, 2), "</b><br>")) %>%
  pull(text) %>%
  paste(collapse = "")

```


Column {data-width=500 .discussion-column}
-------

```{r, echo=FALSE, results='asis'}
# Print the pre-formatted text with HTML bold tags
cat(average_bpm_text)

```
In electronic music, a track's tempo is its pulse – the driving force that sets the mood on the dance floor.  Within the world of house music, even subtle changes in beats per minute (BPM) dramatically alter the atmosphere. Let's dive into three subgenres to see how:

- **Afro House**: Infused with traditional African rhythms, Afro House thrums with an average of 121 BPM. This tempo creates a vibrant yet smooth energy, perfect for dance styles that emphasize fluidity and percussive movement.
- **Deep House**: With a slightly faster pulse (126 BPM on average), Deep House caters to the club scene. Soulful melodies and a driving bassline thrive at this elevated tempo, encouraging sustained, energetic movement.
- **Melodic House**: At an average of 123 BPM, Melodic House finds a middle ground between the other two subgenres. The tempo allows space for both reflective moments and upbeat dance sequences. This complements the genre's focus on emotive melodies and atmospheric soundscapes, creating a vibe that's captivating for both listeners and dancers.

From the vibrant energy of Afro House to the pulsating power of Deep House and the evocative soundscapes of Melodic House,  tempo plays a crucial role in shaping the unique feel of each subgenre.



# Dynamics of Volume: Loudness Across Genres
## Visualization


Column {data-width=750 .plot-column}
-------


```{r}
# Prepare data for plotting
loudness_data <- all_audio_features_df %>%
  select(genre, loudness) %>%
  mutate(genre = factor(genre, levels = c('Afro House', 'Deep House', 'Melodic House')))

# Create a boxplot to compare the loudness distribution
loudness_plot <- ggplot(loudness_data, aes(x = genre, y = loudness, fill = genre)) +
  geom_boxplot() +
  labs(title = "Distribution of Loudness Across Genres",
       x = "Genre",
       y = "Loudness (dB)") +
  scale_fill_manual(values = muted_colors) +
  theme_minimal() +
  theme(legend.position = "none")  # Hide the legend if not necessary

# Print the plot
print(loudness_plot)


```

Column {data-width=500 .discussion-column}
-------

This boxplot shows that Deep House stands out for its consistently loud sound. Producers clearly favor a powerful, attention-grabbing style – perfect for the energy of dance floors and clubs.

Afro House and Melodic House share a more moderate volume level. This suggests these genres value a wider range of dynamics instead of always blasting at maximum volume. It also highlights a shared stylistic approach between them.

The small IQR for Deep House reveals that most tracks have a similar, consistently loud mix, designed for environments where a strong audio presence is needed. Afro and Melodic House, with their wider ranges and outlier tracks, seem to explore a greater variety of sounds. They might include softer, more introspective songs alongside the more energetic ones. This shows how diverse these genres can be, from the rhythmic focus of Afro House to the emotional soundscapes of Melodic House.



# Harmonic Landscape: Decoding Key Signature in 'Breathing' by Ben Böhmer

## Visualisation

Column {data-width=750 .plot-column}
-------

```{r, align='center'}
track_id <- "1MvLmHeLkaNgUScgbUVnWJ"
audio_analysis <- get_tidy_audio_analysis(track_id)

# Select and unnest segments, then select start, duration, and pitches
chroma_data <-
  audio_analysis %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

# Process and plot chroma data
chroma_plot <- 
  chroma_data %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>% 
  compmus_gather_chroma() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(
    x = "Time (s)", 
    y = NULL, 
    fill = "Magnitude", 
    title = "Chroma Features Over Time for \nfor 'Breathing' by Ben Bohmer"
  ) +
  theme_minimal() +
  scale_fill_viridis_c()

# Print the plot
print(chroma_plot)

```

Column {data-width=500 .discussion-column}
-------

The chroma plot of Ben Böhmer's 'Breathing' reveals a fascinating story of how its harmonies work within the key of B minor.  Key signatures set a song's emotional tone, and B minor often brings a sense of melancholy and power – a mood that fits well with 'Breathing'.

As the plot shows, the notes of B minor are strongly emphasized. B, E and G, the core notes of B minor, are especially prominent. Their visual 'peaks' could mark moments of tension and release,  influenced by the song's unusual tempo and rhythm.

Interestingly, the plot also shows notes that fall outside the traditional B minor scale. This suggests that the song uses modal interchange and chromaticism, adding layers of complexity. So, while B minor is the overall 'home' of the track, it also allows for exploration beyond the expected notes.

The unique tempo of 'Breathing' probably influences these harmonic explorations. The unusual rhythm might create unusual timing for chord changes and emphasis on individual notes. This contributes to the song's emotional depth and makes it a standout within Melodic House.



# Echoes of Harmony: Chroma and Timbre in the Pulse of 'Breathing'
## Visualisation

Column {data-width=750 .plot-column}
-------

```{r}
track_audio_analysis <- get_tidy_audio_analysis(track_id) 


# Example adapted for your selected track (assuming 'track_audio_analysis' contains your data)
bzt <- track_audio_analysis |>
  compmus_align(bars, segments) |>
  select(bars) |>
  unnest(bars) |>
  mutate(
    pitches = map(segments, compmus_summarise, pitches, method = "acentre", norm = "manhattan"),
    timbre = map(segments, compmus_summarise, timbre, method = "mean")
  )

# Create self-similarity matrices for both chroma and timbre
ssm_data <- bind_rows(
  bzt |> compmus_self_similarity(pitches, "aitchison") |> mutate(d = d / max(d), type = "Chroma"),
  bzt |> compmus_self_similarity(timbre, "euclidean") |> mutate(d = d / max(d), type = "Timbre")
)

# Plotting
# Assuming ssm_data is already defined and contains the data for plotting
# Assuming ssm_data is already defined and contains the data for plotting
ssm_data |>
  ggplot(
    aes(
      x = xstart + xduration / 2,
      y = ystart + yduration / 2,
      width = xduration,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  facet_wrap(~type) +
  scale_fill_viridis_c(option = "E", guide = "none") +
  theme_minimal() +
  theme(
    strip.text.x = element_text(size = 10),
    plot.title.position = "plot",
      # Use this if you do not want to show any axis ticks
  ) +
  labs(
    title = "Self-Similarity Matrices for Chroma and Timbre\nfor 'Breathing' by Ben Bohmer",
    x = "Time (s)",  # Label for x-axis
    y = "Time (s)"   # Label for y-axis
  ) +
  coord_fixed(ratio = 1)  # This enforces a 1:1 aspect ratio



```

Column {data-width=500 .discussion-column}
-------

These self-similarity matrices reveal how 'Breathing' by Ben Böhmer uses repeating patterns in both its harmony (chroma) and its overall sound (timbre) to create a sense of identity.

The diagonal squares in the chroma matrix show where the same chords or melodies come back throughout the song. This creates a feeling of structure and familiarity, which ties into the overall mood of B minor that we identified earlier.

The timbre matrix focuses on changes in the track's texture. Darker bands mean certain sounds come back, which creates a feeling of unity.  Lighter areas mean something new – maybe a different instrument, a change in rhythm, or a quieter moment.

The most interesting part is where the two patterns overlap. These spots probably mark the song's most memorable moments, where the catchy melody combines with a signature sound.  Where the patterns don't line up is also important – it could be where the song surprises us to create a sense of tension or release.

It's important to remember that 'Breathing' has an unusual tempo. This definitely affects how we hear those repeating chords and sounds. What might feel subtle at one speed could stand out more at another, which could be why these patterns are so easy to spot in these matrices.







# Decoding the Dance Floor: Machine Learning Insights into House Subgenres

Column {data-width=750 .plot-column}
-------

```{r}

# Set seed for reproducibility
set.seed(123)

# Define the target and features
target <- "genre"
selected_features <- c("danceability", "energy", "key", "loudness", "mode", 
                       "speechiness", "acousticness", "instrumentalness", 
                       "liveness", "valence", "tempo")

# Splitting the data into training and testing sets
indexes <- createDataPartition(all_audio_features_df[[target]], p = .8, list = FALSE)
train_data <- all_audio_features_df[indexes, ]
test_data <- all_audio_features_df[-indexes, ]

# Select only the desired features plus the target variable for both sets
train_data_selected <- train_data[, c(target, selected_features)]
test_data_selected <- test_data[, c(target, selected_features)]

# Convert genre to a factor
train_data_selected[[target]] <- as.factor(train_data_selected[[target]])
test_data_selected[[target]] <- as.factor(test_data_selected[[target]])

# Train the Random Forest model on the selected features
rf_model <- randomForest(as.formula(paste(target, "~ .")), data=train_data_selected, 
                         importance=TRUE, ntree=500)


varImpPlot(rf_model, type=2, main="Feature Importance")



```


Column {data-width=500 .discussion-column}
-------

```{r}

# Predictions on the test set
predictions <- predict(rf_model, newdata=test_data_selected)

# Evaluation with confusion matrix
confusion <- confusionMatrix(predictions, test_data_selected[[target]])

# Print the confusion matrix
print(confusion)

# Extract overall accuracy from the confusion matrix
accuracy <- confusion$overall['Accuracy']

```

```{r, fig.height=4, fig.width=5}

# Confusion matrix plot using caret and ggplot2
conf_matrix <- as.table(confusion$table)
ggplot(data = as.data.frame(conf_matrix), 
       aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = Freq), colour = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  geom_text(aes(label = sprintf("%1.0f", Freq)), vjust = 1) +
  labs(x = "Actual", y = "Predicted", fill = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))




```


Column {data-width=500 .discussion-column}
-------
```{r}
cat(sprintf("Accuracy: %.2f%%", accuracy * 100))
```

This analysis employs a Random Forest algorithm to explore genre classification. I've trained my model to differentiate between the nuances of Afro House, Deep House, and Melodic House, drawing insight from a diverse range of audio features. By analyzing elements like tempo, energy, danceability, and more, I seek to pinpoint the factors that determine a song's genre assignment.

The Random Forest model achieves a solid accuracy of 76.36%. This demonstrates its ability to correctly identify genres more often than not, offering a useful tool for music analysis and classification. Removing features based on the feature selection did not improve the accuracy so therefore I have included all of the features.

The feature importance plot reveals the most influential variables in the model's decision-making. Unsurprisingly, tempo stands as a key factor, confirming the importance of BPM in electronic music genres. Interestingly, instrumentalness and speechiness also contribute significantly, highlighting the complex interplay of elements in genre definition.

Overall, the classifier's performance and these insights help us dissect the characteristic elements of house music subgenres. Such understanding could potentially improve music recommendation systems and offer guidance for music production.



# Conclusion


**Introduction of the Corpus**

This study has been a detailed exploration of Afro House, Deep House, and Melodic House.  Using Spotify's music catalog, we've analyzed the rhythms, melodies, and textures that give each genre its flavor.  Our diverse collection of tracks highlighted the energy of Afro House, the soulfulness of Deep House, and the  dreamy soundscapes of Melodic House. Focusing on current music, while acknowledging its limitations, gives a vibrant snapshot of where these genres stand today.

**Harmony in Motion**

Visualizing energy and emotion across the genres revealed their  unique landscapes. Afro House soared to high-energy, while Melodic House often explored quieter moods. This shows how each genre offers a range of listening experiences.

**Key Signatures and Genre Dynamics**

Our analysis found that certain musical keys tend to appear more frequently within each genre. C# added to Afro House's excitement, while G shaped Deep House's sound.  This highlights how subtle choices in music theory impact the overall mood.

**The Heartbeat of House**

Tempo proved itself essential to these genres. Afro House's fast beats get us moving, while Melodic House's moderate tempos invite both introspection and dance.

**Dynamics of Volume**

Deep House stood out for its consistently loud volume, while Afro House and Melodic House used a wider range. This suggests these genres can include both energetic tracks and more mellow moments.

**Harmonic Landscape**

We took a deep dive into 'Breathing', using a chroma plot to show its stunning use of B minor.  Modal interchange and chromaticism added depth to its somber, yet powerful, sound.

**Echoes of Harmony**

Self-similarity matrices revealed recurring patterns in 'Breathing's' harmony and overall sound – the building blocks of its identity. Analyzing these patterns helps us appreciate how  the track creates its distinct journey.

**Decoding the Dance Floor**

Machine learning proved its power! With 76.36% accuracy, our model successfully identified a track's genre based on audio features alone.  Tempo  again stood out as a defining characteristic of electronic music styles.

**Conclusion**

This study has shown how complex and varied house music subgenres can be. By analyzing specific examples, we've gained insights into the patterns that make up these genres. More importantly, this work reminds us that beyond the beats and the keys, house music is about feeling, with the power to move both the soul and the body. This exploration inspires us to keep studying the ever-changing world of house music with a data-driven approach.